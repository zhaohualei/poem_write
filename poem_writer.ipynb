{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, json, codecs\n",
    "import tensorflow as tf\n",
    "from bert4keras.bert import build_bert_model\n",
    "from bert4keras.utils import Tokenizer, load_vocab, parallel_apply\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/opt/developer/wp/wzcq/roberta_wwm/bert_config.json'\n",
    "checkpoint_path = '/opt/developer/wp/wzcq/roberta_wwm/bert_model.ckpt'\n",
    "dict_path = '/opt/developer/wp/wzcq/roberta_wwm/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(token_file):\n",
    "    with open(token_file,\"r\") as f:\n",
    "        token_list = f.readlines()\n",
    "        token_dict = {word.strip():id_ for id_,word in enumerate(token_list)}\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "token_dict = get_token_dict(dict_path)\n",
    "tokenizer = OurTokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./ci/ci.song.1000.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ci_examples(input_file):\n",
    "    \"\"\"Read a tang poet json file into a list \"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "        input_data = json.load(reader)\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        ci = []\n",
    "        rhythmic = entry[\"rhythmic\"]\n",
    "        ci.append(rhythmic+\":\")\n",
    "        s = \"\"\n",
    "        for paragraph in entry[\"paragraphs\"]:   \n",
    "            s += paragraph\n",
    "        ci.append(s)\n",
    "        examples.append(ci)    \n",
    "    return examples     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_ci_examples(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(x):\n",
    "    \"\"\"padding至batch内的最大长度\n",
    "    \"\"\"\n",
    "    ml = max([len(i) for i in x])\n",
    "    return np.array([i + [0] * (ml - len(i)) for i in x])\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    while True:\n",
    "        X, S = [], []\n",
    "        for t,d in data:\n",
    "            x, s = tokenizer.encode(t,d)\n",
    "            X.append(x)\n",
    "            S.append(s)\n",
    "            if len(X) == batch_size:\n",
    "                X = padding(X)\n",
    "                S = padding(S)\n",
    "                yield [X, S], None\n",
    "                X, S = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> searching: bert/embeddings/word_embeddings, found name: bert/embeddings/word_embeddings\n",
      "==> searching: bert/embeddings/position_embeddings, found name: bert/embeddings/position_embeddings\n",
      "==> searching: bert/embeddings/token_type_embeddings, found name: bert/embeddings/token_type_embeddings\n",
      "==> searching: bert/embeddings/LayerNorm/gamma, found name: bert/embeddings/LayerNorm/gamma\n",
      "==> searching: bert/embeddings/LayerNorm/beta, found name: bert/embeddings/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_0/attention/self/query/kernel, found name: bert/encoder/layer_0/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/query/bias, found name: bert/encoder/layer_0/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_0/attention/self/key/kernel, found name: bert/encoder/layer_0/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/key/bias, found name: bert/encoder/layer_0/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_0/attention/self/value/kernel, found name: bert/encoder/layer_0/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/value/bias, found name: bert/encoder/layer_0/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_0/attention/output/dense/kernel, found name: bert/encoder/layer_0/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/output/dense/bias, found name: bert/encoder/layer_0/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_0/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/beta, found name: bert/encoder/layer_0/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_0/intermediate/dense/kernel, found name: bert/encoder/layer_0/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/intermediate/dense/bias, found name: bert/encoder/layer_0/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_0/output/dense/kernel, found name: bert/encoder/layer_0/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/output/dense/bias, found name: bert/encoder/layer_0/output/dense/bias\n",
      "==> searching: bert/encoder/layer_0/output/LayerNorm/gamma, found name: bert/encoder/layer_0/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_0/output/LayerNorm/beta, found name: bert/encoder/layer_0/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_1/attention/self/query/kernel, found name: bert/encoder/layer_1/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/query/bias, found name: bert/encoder/layer_1/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_1/attention/self/key/kernel, found name: bert/encoder/layer_1/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/key/bias, found name: bert/encoder/layer_1/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_1/attention/self/value/kernel, found name: bert/encoder/layer_1/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/value/bias, found name: bert/encoder/layer_1/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_1/attention/output/dense/kernel, found name: bert/encoder/layer_1/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/output/dense/bias, found name: bert/encoder/layer_1/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_1/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/beta, found name: bert/encoder/layer_1/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_1/intermediate/dense/kernel, found name: bert/encoder/layer_1/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/intermediate/dense/bias, found name: bert/encoder/layer_1/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_1/output/dense/kernel, found name: bert/encoder/layer_1/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/output/dense/bias, found name: bert/encoder/layer_1/output/dense/bias\n",
      "==> searching: bert/encoder/layer_1/output/LayerNorm/gamma, found name: bert/encoder/layer_1/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_1/output/LayerNorm/beta, found name: bert/encoder/layer_1/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_2/attention/self/query/kernel, found name: bert/encoder/layer_2/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/query/bias, found name: bert/encoder/layer_2/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_2/attention/self/key/kernel, found name: bert/encoder/layer_2/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/key/bias, found name: bert/encoder/layer_2/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_2/attention/self/value/kernel, found name: bert/encoder/layer_2/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/value/bias, found name: bert/encoder/layer_2/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_2/attention/output/dense/kernel, found name: bert/encoder/layer_2/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/output/dense/bias, found name: bert/encoder/layer_2/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_2/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/beta, found name: bert/encoder/layer_2/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_2/intermediate/dense/kernel, found name: bert/encoder/layer_2/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/intermediate/dense/bias, found name: bert/encoder/layer_2/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_2/output/dense/kernel, found name: bert/encoder/layer_2/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/output/dense/bias, found name: bert/encoder/layer_2/output/dense/bias\n",
      "==> searching: bert/encoder/layer_2/output/LayerNorm/gamma, found name: bert/encoder/layer_2/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_2/output/LayerNorm/beta, found name: bert/encoder/layer_2/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_3/attention/self/query/kernel, found name: bert/encoder/layer_3/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/query/bias, found name: bert/encoder/layer_3/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_3/attention/self/key/kernel, found name: bert/encoder/layer_3/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/key/bias, found name: bert/encoder/layer_3/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_3/attention/self/value/kernel, found name: bert/encoder/layer_3/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/value/bias, found name: bert/encoder/layer_3/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_3/attention/output/dense/kernel, found name: bert/encoder/layer_3/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/output/dense/bias, found name: bert/encoder/layer_3/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_3/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/beta, found name: bert/encoder/layer_3/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_3/intermediate/dense/kernel, found name: bert/encoder/layer_3/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/intermediate/dense/bias, found name: bert/encoder/layer_3/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_3/output/dense/kernel, found name: bert/encoder/layer_3/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/output/dense/bias, found name: bert/encoder/layer_3/output/dense/bias\n",
      "==> searching: bert/encoder/layer_3/output/LayerNorm/gamma, found name: bert/encoder/layer_3/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_3/output/LayerNorm/beta, found name: bert/encoder/layer_3/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_4/attention/self/query/kernel, found name: bert/encoder/layer_4/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/query/bias, found name: bert/encoder/layer_4/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_4/attention/self/key/kernel, found name: bert/encoder/layer_4/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/key/bias, found name: bert/encoder/layer_4/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_4/attention/self/value/kernel, found name: bert/encoder/layer_4/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/value/bias, found name: bert/encoder/layer_4/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_4/attention/output/dense/kernel, found name: bert/encoder/layer_4/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/output/dense/bias, found name: bert/encoder/layer_4/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_4/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/beta, found name: bert/encoder/layer_4/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_4/intermediate/dense/kernel, found name: bert/encoder/layer_4/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/intermediate/dense/bias, found name: bert/encoder/layer_4/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_4/output/dense/kernel, found name: bert/encoder/layer_4/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/output/dense/bias, found name: bert/encoder/layer_4/output/dense/bias\n",
      "==> searching: bert/encoder/layer_4/output/LayerNorm/gamma, found name: bert/encoder/layer_4/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_4/output/LayerNorm/beta, found name: bert/encoder/layer_4/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_5/attention/self/query/kernel, found name: bert/encoder/layer_5/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/query/bias, found name: bert/encoder/layer_5/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_5/attention/self/key/kernel, found name: bert/encoder/layer_5/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/key/bias, found name: bert/encoder/layer_5/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_5/attention/self/value/kernel, found name: bert/encoder/layer_5/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/value/bias, found name: bert/encoder/layer_5/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_5/attention/output/dense/kernel, found name: bert/encoder/layer_5/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/output/dense/bias, found name: bert/encoder/layer_5/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_5/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/beta, found name: bert/encoder/layer_5/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_5/intermediate/dense/kernel, found name: bert/encoder/layer_5/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/intermediate/dense/bias, found name: bert/encoder/layer_5/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_5/output/dense/kernel, found name: bert/encoder/layer_5/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/output/dense/bias, found name: bert/encoder/layer_5/output/dense/bias\n",
      "==> searching: bert/encoder/layer_5/output/LayerNorm/gamma, found name: bert/encoder/layer_5/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_5/output/LayerNorm/beta, found name: bert/encoder/layer_5/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_6/attention/self/query/kernel, found name: bert/encoder/layer_6/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/query/bias, found name: bert/encoder/layer_6/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_6/attention/self/key/kernel, found name: bert/encoder/layer_6/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/key/bias, found name: bert/encoder/layer_6/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_6/attention/self/value/kernel, found name: bert/encoder/layer_6/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/value/bias, found name: bert/encoder/layer_6/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_6/attention/output/dense/kernel, found name: bert/encoder/layer_6/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/output/dense/bias, found name: bert/encoder/layer_6/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_6/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/beta, found name: bert/encoder/layer_6/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_6/intermediate/dense/kernel, found name: bert/encoder/layer_6/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/intermediate/dense/bias, found name: bert/encoder/layer_6/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_6/output/dense/kernel, found name: bert/encoder/layer_6/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/output/dense/bias, found name: bert/encoder/layer_6/output/dense/bias\n",
      "==> searching: bert/encoder/layer_6/output/LayerNorm/gamma, found name: bert/encoder/layer_6/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_6/output/LayerNorm/beta, found name: bert/encoder/layer_6/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_7/attention/self/query/kernel, found name: bert/encoder/layer_7/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/query/bias, found name: bert/encoder/layer_7/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_7/attention/self/key/kernel, found name: bert/encoder/layer_7/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/key/bias, found name: bert/encoder/layer_7/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_7/attention/self/value/kernel, found name: bert/encoder/layer_7/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/value/bias, found name: bert/encoder/layer_7/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_7/attention/output/dense/kernel, found name: bert/encoder/layer_7/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/output/dense/bias, found name: bert/encoder/layer_7/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_7/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/beta, found name: bert/encoder/layer_7/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_7/intermediate/dense/kernel, found name: bert/encoder/layer_7/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/intermediate/dense/bias, found name: bert/encoder/layer_7/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_7/output/dense/kernel, found name: bert/encoder/layer_7/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/output/dense/bias, found name: bert/encoder/layer_7/output/dense/bias\n",
      "==> searching: bert/encoder/layer_7/output/LayerNorm/gamma, found name: bert/encoder/layer_7/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_7/output/LayerNorm/beta, found name: bert/encoder/layer_7/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_8/attention/self/query/kernel, found name: bert/encoder/layer_8/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/query/bias, found name: bert/encoder/layer_8/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_8/attention/self/key/kernel, found name: bert/encoder/layer_8/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/key/bias, found name: bert/encoder/layer_8/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_8/attention/self/value/kernel, found name: bert/encoder/layer_8/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/value/bias, found name: bert/encoder/layer_8/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_8/attention/output/dense/kernel, found name: bert/encoder/layer_8/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/output/dense/bias, found name: bert/encoder/layer_8/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_8/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/beta, found name: bert/encoder/layer_8/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_8/intermediate/dense/kernel, found name: bert/encoder/layer_8/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/intermediate/dense/bias, found name: bert/encoder/layer_8/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_8/output/dense/kernel, found name: bert/encoder/layer_8/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/output/dense/bias, found name: bert/encoder/layer_8/output/dense/bias\n",
      "==> searching: bert/encoder/layer_8/output/LayerNorm/gamma, found name: bert/encoder/layer_8/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_8/output/LayerNorm/beta, found name: bert/encoder/layer_8/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_9/attention/self/query/kernel, found name: bert/encoder/layer_9/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/query/bias, found name: bert/encoder/layer_9/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_9/attention/self/key/kernel, found name: bert/encoder/layer_9/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/key/bias, found name: bert/encoder/layer_9/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_9/attention/self/value/kernel, found name: bert/encoder/layer_9/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/value/bias, found name: bert/encoder/layer_9/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_9/attention/output/dense/kernel, found name: bert/encoder/layer_9/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/output/dense/bias, found name: bert/encoder/layer_9/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_9/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/beta, found name: bert/encoder/layer_9/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_9/intermediate/dense/kernel, found name: bert/encoder/layer_9/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/intermediate/dense/bias, found name: bert/encoder/layer_9/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_9/output/dense/kernel, found name: bert/encoder/layer_9/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/output/dense/bias, found name: bert/encoder/layer_9/output/dense/bias\n",
      "==> searching: bert/encoder/layer_9/output/LayerNorm/gamma, found name: bert/encoder/layer_9/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_9/output/LayerNorm/beta, found name: bert/encoder/layer_9/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_10/attention/self/query/kernel, found name: bert/encoder/layer_10/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/query/bias, found name: bert/encoder/layer_10/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_10/attention/self/key/kernel, found name: bert/encoder/layer_10/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/key/bias, found name: bert/encoder/layer_10/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_10/attention/self/value/kernel, found name: bert/encoder/layer_10/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/value/bias, found name: bert/encoder/layer_10/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_10/attention/output/dense/kernel, found name: bert/encoder/layer_10/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/output/dense/bias, found name: bert/encoder/layer_10/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_10/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/beta, found name: bert/encoder/layer_10/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_10/intermediate/dense/kernel, found name: bert/encoder/layer_10/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/intermediate/dense/bias, found name: bert/encoder/layer_10/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_10/output/dense/kernel, found name: bert/encoder/layer_10/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/output/dense/bias, found name: bert/encoder/layer_10/output/dense/bias\n",
      "==> searching: bert/encoder/layer_10/output/LayerNorm/gamma, found name: bert/encoder/layer_10/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_10/output/LayerNorm/beta, found name: bert/encoder/layer_10/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_11/attention/self/query/kernel, found name: bert/encoder/layer_11/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/query/bias, found name: bert/encoder/layer_11/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_11/attention/self/key/kernel, found name: bert/encoder/layer_11/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/key/bias, found name: bert/encoder/layer_11/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_11/attention/self/value/kernel, found name: bert/encoder/layer_11/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/value/bias, found name: bert/encoder/layer_11/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_11/attention/output/dense/kernel, found name: bert/encoder/layer_11/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/output/dense/bias, found name: bert/encoder/layer_11/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_11/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/beta, found name: bert/encoder/layer_11/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_11/intermediate/dense/kernel, found name: bert/encoder/layer_11/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/intermediate/dense/bias, found name: bert/encoder/layer_11/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_11/output/dense/kernel, found name: bert/encoder/layer_11/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/output/dense/bias, found name: bert/encoder/layer_11/output/dense/bias\n",
      "==> searching: bert/encoder/layer_11/output/LayerNorm/gamma, found name: bert/encoder/layer_11/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_11/output/LayerNorm/beta, found name: bert/encoder/layer_11/output/LayerNorm/beta\n",
      "==> searching: cls/predictions/transform/dense/kernel, found name: cls/predictions/transform/dense/kernel\n",
      "==> searching: cls/predictions/transform/dense/bias, found name: cls/predictions/transform/dense/bias\n",
      "==> searching: cls/predictions/transform/LayerNorm/gamma, found name: cls/predictions/transform/LayerNorm/gamma\n",
      "==> searching: cls/predictions/transform/LayerNorm/beta, found name: cls/predictions/transform/LayerNorm/beta\n",
      "==> searching: cls/predictions/output_bias, found name: cls/predictions/output_bias\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Sequence-Mask (Lambda)          (None, None)         0           Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Attention-Mask (Lambda)         (None, None, None)   0           Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 768)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 768)    0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 768)    0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, None, 768)    0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, None, 768)    0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, None, 768)    0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, None, 768)    0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, None, 768)    0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, None, 768)    0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, None, 768)    0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, None, 768)    1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, None, 768)    0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, None, 768)    1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, None, 768)    0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, None, 768)    1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Proba (EmbeddingDense)      (None, None, 21128)  21128       MLM-Norm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 102,290,312\n",
      "Trainable params: 102,290,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/developer/wp/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output MLM-Proba missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to MLM-Proba.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "model = build_bert_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    application='seq2seq',\n",
    "    # 只保留keep_words中的字，精简原字表\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 交叉熵作为loss，并mask掉输入部分的预测\n",
    "y_in = model.input[0][:, 1:]  # 目标tokens\n",
    "y_mask = model.input[1][:, 1:]\n",
    "y = model.output[:, :-1]  # 预测tokens，预测与目标错开一位\n",
    "cross_entropy = K.sparse_categorical_crossentropy(y_in, y)\n",
    "cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)\n",
    "\n",
    "model.add_loss(cross_entropy)\n",
    "model.compile(optimizer=Adam(1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sent(s, topk=3):\n",
    "    \"\"\"beam search解码\n",
    "    每次只保留topk个最优候选结果；如果topk=1，那么就是贪心搜索\n",
    "    \"\"\"\n",
    "    token_ids, segment_ids = tokenizer.encode(s[:max_input_len])\n",
    "    target_ids = [[] for _ in range(topk)]  # 候选答案id\n",
    "    target_scores = [0] * topk  # 候选答案分数\n",
    "    for i in range(max_output_len):  # 强制要求输出不超过max_output_len字\n",
    "        _target_ids = [token_ids + t for t in target_ids]\n",
    "        _segment_ids = [segment_ids + [1] * len(t) for t in target_ids]\n",
    "        _probas = model.predict([_target_ids, _segment_ids\n",
    "                                 ])[:, -1, 3:]  # 直接忽略[PAD], [UNK], [CLS]\n",
    "        _log_probas = np.log(_probas + 1e-6)  # 取对数，方便计算\n",
    "        _topk_arg = _log_probas.argsort(axis=1)[:, -topk:]  # 每一项选出topk\n",
    "        _candidate_ids, _candidate_scores = [], []\n",
    "        for j, (ids, sco) in enumerate(zip(target_ids, target_scores)):\n",
    "            # 预测第一个字的时候，输入的topk事实上都是同一个，\n",
    "            # 所以只需要看第一个，不需要遍历后面的。\n",
    "            if i == 0 and j > 0:\n",
    "                continue\n",
    "            for k in _topk_arg[j]:\n",
    "                _candidate_ids.append(ids + [k + 3])\n",
    "                _candidate_scores.append(sco + _log_probas[j][k])\n",
    "        _topk_arg = np.argsort(_candidate_scores)[-topk:]  # 从中选出新的topk\n",
    "        target_ids = [_candidate_ids[k] for k in _topk_arg]\n",
    "        target_scores = [_candidate_scores[k] for k in _topk_arg]\n",
    "        best_one = np.argmax(target_scores)\n",
    "        if target_ids[best_one][-1] == 102:\n",
    "            return tokenizer.decode(target_ids[best_one])\n",
    "    # 如果max_output_len字都找不到结束符，直接返回\n",
    "    return tokenizer.decode(target_ids[np.argmax(target_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成词: 楼头上有三冬鼓。何须抵死催人去。上马苦匆匆。琵琶曲未终。回头肠断处。却更廉纤雨。漫道玉为堂。玉堂今夜长。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def just_show():\n",
    "    s1 = \"菩萨蛮:\"\n",
    "#     s2 = \"踏莎行:\"\n",
    "#     for s in [s1, s2]:\n",
    "    print('生成词:', gen_sent(s1))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 保存最优\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save_weights('./best_model.weights')\n",
    "        # 演示效果\n",
    "        just_show()\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "steps_per_epoch = 1000  \n",
    "epochs = 100\n",
    "max_input_len = 32\n",
    "max_output_len = 256\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "#     evaluator = Evaluate()\n",
    "\n",
    "#     model.fit_generator(data_generator(),\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         epochs=epochs,\n",
    "#                         callbacks=[evaluator])\n",
    "   just_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
